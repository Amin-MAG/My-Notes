{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PoS Implementation\n",
        "Mohammad Amin Ghasvari Jahromi - 97521432"
      ],
      "metadata": {
        "id": "r_gjVoaPQt7k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDTYLpd7QtKq"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rxEc_HuUUYA4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "import pprint\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from random import shuffle\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_7McOVQtKv"
      },
      "source": [
        "# Datasets download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O4WyhH9WL2f",
        "outputId": "0bf0c620-6375-477c-df9c-9e5e8e40ba7c",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm9Wr4koQtKx"
      },
      "source": [
        "# Tagged sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I select one of these tagged sentences to see the structure."
      ],
      "metadata": {
        "id": "eOar6ZZ-dABg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GYqC2mfUj_S",
        "outputId": "77610892-8dcd-47dd-ec4e-c2b4db8dfd36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "tagged_sentences = list(treebank.tagged_sents())\n",
        "\n",
        "# For exapmle\n",
        "print(tagged_sentences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Test and Train datasets\n",
        "I shuffled the tagged sentences and consider the first %80 of this list for training and the rest of that for testing."
      ],
      "metadata": {
        "id": "0kYhH7R0RqfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffling\n",
        "shuffle(tagged_sentences)\n",
        "length_of_tagged_sentences = len(tagged_sentences)\n",
        "\n",
        "# Split the train and test datasets\n",
        "train_data, test_data = tagged_sentences[:int(length_of_tagged_sentences * 0.8)], tagged_sentences[int(length_of_tagged_sentences * 0.8):]\n",
        "\n",
        "# Show some statistics\n",
        "print(f\"All the data: {length_of_tagged_sentences}\")\n",
        "print(f\"Train data: {len(train_data)}\")\n",
        "print(f\"Test data: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkGiL5tBQz3X",
        "outputId": "c4d81f75-ef84-4342-88d8-417b4b8474e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All the data: 3914\n",
            "Train data: 3131\n",
            "Test data: 783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined a function to create the X and Y by having tagged"
      ],
      "metadata": {
        "id": "cai7PoD9fFuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(tagged_sentences):\n",
        "  X, Y = [], []      \n",
        "  for tagged in tagged_sentences:         \n",
        "    untag_sen = [w for w, t in tagged]  \n",
        "    for index in range(len(tagged)):\n",
        "      X.append(features(untag_sen, index))\n",
        "      Y.append(tagged[index][1])\n",
        "  \n",
        "  return X, Y  "
      ],
      "metadata": {
        "id": "pFZ1ASaAfGB2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features\n",
        "It's time to define some features for our input dataset."
      ],
      "metadata": {
        "id": "zqWeAVmvS30-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def features(sentence, index):\n",
        "  return {\n",
        "    'word': sentence[index], \n",
        "    'len': len(sentence[index]),\n",
        "\n",
        "    # Position\n",
        "    'is_first': index == 0, \n",
        "    'is_last': index == len(sentence) - 1,  \n",
        "    \n",
        "    # Other words\n",
        "    'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "    'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],   \n",
        "    \n",
        "    # Suffix & Prefix  \n",
        "    'prefix-1': sentence[index][0],       \n",
        "    'prefix-2': sentence[index][:2],      \n",
        "    'prefix-3': sentence[index][:3],       \n",
        "    'suffix-1': sentence[index][-1],     \n",
        "    'suffix-2': sentence[index][-2:],      \n",
        "    'suffix-3': sentence[index][-3:],     \n",
        "\n",
        "    # Type of characters\n",
        "    'is_numeric': sentence[index].isdigit(),\n",
        "    'is_punc': any([sentence[index] == p for p in punctuation]),\n",
        "\n",
        "    # Captalization\n",
        "    'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "    'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
        "    'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "    'is_all_lower': sentence[index].lower() == sentence[index]\n",
        "  }  \n"
      ],
      "metadata": {
        "id": "kHUfHdsTS3g2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "First of all, let's create our dataset."
      ],
      "metadata": {
        "id": "aFfXBtiVfSsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = create_dataset(train_data)\n",
        "# Print a sample\n",
        "print(x_train[30])\n",
        "print(y_train[30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgzWlYLMW2Or",
        "outputId": "1fe3c82f-a4a8-4c74-a1a1-fe53fe3b9ca8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': \"'s\", 'len': 2, 'is_first': False, 'is_last': False, 'prev_word': 'Japan', 'next_word': '.', 'prefix-1': \"'\", 'prefix-2': \"'s\", 'prefix-3': \"'s\", 'suffix-1': 's', 'suffix-2': \"'s\", 'suffix-3': \"'s\", 'is_numeric': False, 'is_punc': False, 'is_capitalized': True, 'capitals_inside': False, 'is_all_caps': False, 'is_all_lower': True}\n",
            "POS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Sklearn package to create a pipeline. I used the DictVectorizer which can vectorize the features for each sentence. DecisionTreeClassifier is the classifier that is going to train our data.\n",
        "\n",
        "I used the first 20000 x's of my train dataset since I didn't have enough amount of memory to train all of the train datasets."
      ],
      "metadata": {
        "id": "bvlufZAUUYhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Pipeline([('vectorizer', DictVectorizer(sparse=False)),('classifier', DecisionTreeClassifier(criterion='entropy'))])\n",
        "classifier.fit(x_train[:20000], y_train[:20000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWjfDfI5Yx9V",
        "outputId": "328f8377-7f23-4ade-beee-2da5f751e9f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', DictVectorizer(sparse=False)),\n",
              "                ('classifier', DecisionTreeClassifier(criterion='entropy'))])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model\n",
        "I created the train dataset to test the provided model."
      ],
      "metadata": {
        "id": "lmQzGPE1ZOnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = create_dataset(test_data)\n",
        "\n",
        "print(f\"Accuracy: %{classifier.score(x_test, y_test) * 100}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuueLqvQZPLA",
        "outputId": "6888d7e3-10fe-42ca-e3b8-20a43c192703"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: %92.39497827136648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify\n",
        "Here is an example of the classification."
      ],
      "metadata": {
        "id": "RvXo_9o0ZoMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag(sentence):\n",
        "  tags = classifier.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "  return list(zip(sentence, tags)) \n",
        "\n",
        "print(pos_tag(word_tokenize('Hello everybody, My name is Amin.')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSg9L0ljZohf",
        "outputId": "b764ac13-852a-4a3f-df47-12abb56747d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'NNP'), ('everybody', 'NN'), (',', ','), ('My', 'NNP'), ('name', 'NN'), ('is', 'VBZ'), ('Amin', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PoS_Tagger.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}