{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34kSvaXZejxP"
      },
      "source": [
        "# **HW2 - Text Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: `Mohammad Amin Ghasvari Jahromi`\n",
        "\n",
        "Student Number: `97521432`"
      ],
      "metadata": {
        "id": "yLDUi24LMTnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Imports"
      ],
      "metadata": {
        "id": "FEC18pjNMQa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I Imported the dataset, numpy, and nltk packages."
      ],
      "metadata": {
        "id": "WD86pe0ZTrxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "import collections\n",
        "from copy import deepcopy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import *\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n",
        "# nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dagCqRvAwLi",
        "outputId": "8093fd23-e30f-4f81-e364-7805fbba2d6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAnj55oxdpiQ"
      },
      "source": [
        "## 1. IMDB Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried to read from data set and print out some of these samples."
      ],
      "metadata": {
        "id": "k3jqZjcoTyCi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElC24F9DdRxq",
        "outputId": "3cdfbe21-b62e-4efb-deaf-f8e642284177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the IMDB dataset...\n",
            "Count train sequences: 25000\n",
            "Count test sequences: 25000\n",
            "\n",
            "Classes: \n",
            "[0 1]\n",
            "\n",
            "Review-Example:\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Loading the dataset\n",
        "print('Loading the IMDB dataset...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
        "print(f'Count train sequences: {len(x_train)}')\n",
        "print(f'Count test sequences: {len(x_test)}')\n",
        "print()\n",
        "\n",
        "# Number of classes\n",
        "print(\"Classes: \")\n",
        "print(np.unique(y_train))\n",
        "print()\n",
        "\n",
        "# A Review example\n",
        "print(\"Review-Example:\")\n",
        "print(x_train[0])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpXc193Vd30r"
      },
      "source": [
        "## 2. Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Defining some useful variables and functions"
      ],
      "metadata": {
        "id": "z9rRDRajLuoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are 2 functions. The first one `index_sen` is for converting index samples to word samples. On the other hand `sen_index` is for converting sentence samples to index samples."
      ],
      "metadata": {
        "id": "uin-J1RTT34g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "invtr_word_index = {i: word for word, i in word_index.items()}\n",
        "\n",
        "\n",
        "def index_sen(iws, indexes_lists):\n",
        "  \"\"\"\n",
        "  indexes_lists: list of arrays that contain indexes\n",
        "  Returns lists of the sentence based on given indexes\n",
        "  \"\"\"\n",
        "\n",
        "  sentences = []\n",
        "  for indexes in indexes_lists:\n",
        "    words = [iws.get(idx) for idx in indexes]\n",
        "    sentences.append([word for word in words if word is not None])\n",
        "\n",
        "  return np.array(sentences, dtype=object)\n",
        "\n",
        "def sen_index(ws, sentences):\n",
        "  \"\"\"\n",
        "  sentences: list of arrays that contain words\n",
        "  Return lists of the indexes based on given sentence.\n",
        "  \"\"\"\n",
        "\n",
        "  indexes_list = []\n",
        "  for sentence in sentences:\n",
        "    indexes = [ws.get(word) for word in sentence]\n",
        "    indexes_list.append([index for index in indexes if index is not None])\n",
        "\n",
        "  return np.array(indexes_list, dtype=object)\n",
        "  \n",
        "  \n",
        "# Example for testing index_sen() and sen_index()\n",
        "print(x_train[200])\n",
        "print(index_sen(invtr_word_index, [x_train[200]]))\n",
        "print(sen_index(word_index, [index_sen(invtr_word_index, [x_train[200]])[0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR81vwNUIIXH",
        "outputId": "c6274db9-6c50-4086-e6ff-0ba35c759acb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 9, 6, 227, 196, 241, 634, 891, 234, 21, 12, 69, 6, 6, 176, 7, 4, 804, 4658, 2999, 667, 11, 12, 11, 85, 715, 6, 176, 7, 1565, 8, 1108, 10, 10, 12, 16, 1844, 78177, 33, 211, 21, 69, 49, 2009, 905, 388, 99, 17874, 125, 34, 6, 25904, 1274, 33, 4, 130, 7, 4, 22, 15, 16, 6424, 8, 650, 1069, 14, 22, 9, 44, 4609, 153, 154, 4, 318, 302, 1051, 23, 14, 22, 122, 6, 2093, 292, 10, 10, 723, 8721, 5, 11200, 9728, 71, 1344, 1576, 156, 11, 68, 251, 5, 36, 92, 4363, 133, 199, 743, 976, 354, 4, 64, 439, 9, 3059, 17, 32, 4, 12445, 26, 256, 34, 85847, 5, 49, 7, 98, 40, 2345, 9844, 43, 92, 168, 147, 474, 40, 8, 67, 6, 796, 97, 7, 14, 20, 19, 32, 2188, 156, 24, 18, 6090, 1007, 21, 8, 331, 97, 4, 65, 168, 5, 481, 53, 3084]\n",
            "[['the' 'as' 'it' 'is' 'far' 'both' 'am' \"i'll\" 'unless' 'since' 'not'\n",
            "  'that' 'me' 'is' 'is' 'quite' 'br' 'of' 'dialog' 'depicts' 'stupidity'\n",
            "  'robert' 'this' 'that' 'this' 'because' 'jack' 'is' 'quite' 'br'\n",
            "  'provides' 'in' 'island' 'i' 'i' 'that' 'with' 'plots' 'indicitive'\n",
            "  'they' 'gets' 'not' 'me' 'good' 'shallow' 'earlier' 'understand'\n",
            "  'movies' 'suckers' 'better' 'who' 'is' 'tupac' 'catch' 'they' 'of'\n",
            "  'here' 'br' 'of' 'you' 'for' 'with' 'clash' 'in' 'change' 'cartoon'\n",
            "  'as' 'you' 'it' 'has' 'questionable' 'actors' 'work' 'of' 'excellent'\n",
            "  'instead' 'ended' 'are' 'as' 'you' 'off' 'is' 'comparison' 'together'\n",
            "  'i' 'i' 'whether' 'zeta' 'to' 'hitchhiker' 'forum' 'than' 'plan'\n",
            "  'tragic' 'before' 'this' 'were' 'hard' 'to' 'from' 'then' 'abysmal'\n",
            "  'scene' 'give' 'within' 'telling' 'boring' 'of' 'see' \"she's\" 'it'\n",
            "  'glass' 'movie' 'an' 'of' 'culprit' 'he' 'anyone' 'who' 'nananana' 'to'\n",
            "  'good' 'br' 'any' 'just' 'hanging' 'marvellous' 'out' 'then' 'few'\n",
            "  'now' 'final' 'just' 'in' 'can' 'is' 'sister' 'could' 'br' 'as' 'on'\n",
            "  'film' 'an' 'levels' 'before' 'his' 'but' 'regards' 'western' 'not'\n",
            "  'in' 'else' 'could' 'of' 'their' 'few' 'to' 'totally' 'up' 'spin']]\n",
            "[[1 14 9 6 227 196 241 634 891 234 21 12 69 6 6 176 7 4 804 4658 2999 667\n",
            "  11 12 11 85 715 6 176 7 1565 8 1108 10 10 12 16 1844 78177 33 211 21 69\n",
            "  49 2009 905 388 99 17874 125 34 6 25904 1274 33 4 130 7 4 22 15 16 6424\n",
            "  8 650 1069 14 22 9 44 4609 153 154 4 318 302 1051 23 14 22 122 6 2093\n",
            "  292 10 10 723 8721 5 11200 9728 71 1344 1576 156 11 68 251 5 36 92 4363\n",
            "  133 199 743 976 354 4 64 439 9 3059 17 32 4 12445 26 256 34 85847 5 49\n",
            "  7 98 40 2345 9844 43 92 168 147 474 40 8 67 6 796 97 7 14 20 19 32 2188\n",
            "  156 24 18 6090 1007 21 8 331 97 4 65 168 5 481 53 3084]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Decoding all train data"
      ],
      "metadata": {
        "id": "BeifHtqBNEiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train sample 200 as a test\n",
        "print(x_train[200])\n",
        "print(x_test[200])\n",
        "\n",
        "# Convert to real words\n",
        "sentences = index_sen(invtr_word_index, x_train)\n",
        "print(sentences[200])\n",
        "test_sentences = index_sen(invtr_word_index, x_test)\n",
        "print(test_sentences[200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF4d-LFVNIJz",
        "outputId": "1b447470-943c-4245-8c7c-ba5da7b51e01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 9, 6, 227, 196, 241, 634, 891, 234, 21, 12, 69, 6, 6, 176, 7, 4, 804, 4658, 2999, 667, 11, 12, 11, 85, 715, 6, 176, 7, 1565, 8, 1108, 10, 10, 12, 16, 1844, 78177, 33, 211, 21, 69, 49, 2009, 905, 388, 99, 17874, 125, 34, 6, 25904, 1274, 33, 4, 130, 7, 4, 22, 15, 16, 6424, 8, 650, 1069, 14, 22, 9, 44, 4609, 153, 154, 4, 318, 302, 1051, 23, 14, 22, 122, 6, 2093, 292, 10, 10, 723, 8721, 5, 11200, 9728, 71, 1344, 1576, 156, 11, 68, 251, 5, 36, 92, 4363, 133, 199, 743, 976, 354, 4, 64, 439, 9, 3059, 17, 32, 4, 12445, 26, 256, 34, 85847, 5, 49, 7, 98, 40, 2345, 9844, 43, 92, 168, 147, 474, 40, 8, 67, 6, 796, 97, 7, 14, 20, 19, 32, 2188, 156, 24, 18, 6090, 1007, 21, 8, 331, 97, 4, 65, 168, 5, 481, 53, 3084]\n",
            "[1, 92, 30, 4460, 14, 218, 246, 160, 1458, 463, 7, 4, 539, 39, 4172, 834, 4470, 8, 30, 4, 785, 16117, 768, 216, 8, 703, 8, 6585, 17, 76, 16769, 17, 614, 512, 151, 4, 3935, 203, 1467, 900, 14, 20, 9, 6, 1528, 35, 1557, 6823, 15, 47, 23464, 61, 113, 39, 4, 561, 13, 2984, 523, 725, 12, 642, 10267, 206, 3015, 5, 6248, 2475, 30455, 9, 6, 8874, 23203, 3075, 852, 1147, 6, 8659, 2184, 38647, 356, 10, 10, 570, 285, 25, 26, 399, 5, 521, 46, 8, 851, 42, 128, 246, 818, 14, 20, 1241]\n",
            "['the', 'as', 'it', 'is', 'far', 'both', 'am', \"i'll\", 'unless', 'since', 'not', 'that', 'me', 'is', 'is', 'quite', 'br', 'of', 'dialog', 'depicts', 'stupidity', 'robert', 'this', 'that', 'this', 'because', 'jack', 'is', 'quite', 'br', 'provides', 'in', 'island', 'i', 'i', 'that', 'with', 'plots', 'indicitive', 'they', 'gets', 'not', 'me', 'good', 'shallow', 'earlier', 'understand', 'movies', 'suckers', 'better', 'who', 'is', 'tupac', 'catch', 'they', 'of', 'here', 'br', 'of', 'you', 'for', 'with', 'clash', 'in', 'change', 'cartoon', 'as', 'you', 'it', 'has', 'questionable', 'actors', 'work', 'of', 'excellent', 'instead', 'ended', 'are', 'as', 'you', 'off', 'is', 'comparison', 'together', 'i', 'i', 'whether', 'zeta', 'to', 'hitchhiker', 'forum', 'than', 'plan', 'tragic', 'before', 'this', 'were', 'hard', 'to', 'from', 'then', 'abysmal', 'scene', 'give', 'within', 'telling', 'boring', 'of', 'see', \"she's\", 'it', 'glass', 'movie', 'an', 'of', 'culprit', 'he', 'anyone', 'who', 'nananana', 'to', 'good', 'br', 'any', 'just', 'hanging', 'marvellous', 'out', 'then', 'few', 'now', 'final', 'just', 'in', 'can', 'is', 'sister', 'could', 'br', 'as', 'on', 'film', 'an', 'levels', 'before', 'his', 'but', 'regards', 'western', 'not', 'in', 'else', 'could', 'of', 'their', 'few', 'to', 'totally', 'up', 'spin']\n",
            "['the', 'then', 'at', 'namely', 'as', 'interesting', 'worst', 'funny', 'lover', 'despite', 'br', 'of', 'decent', 'or', 'punk', 'dance', 'transfer', 'in', 'at', 'of', 'clear', 'sickeningly', 'ways', 'saw', 'in', 'cheap', 'in', 'freaky', 'movie', 'get', 'tactic', 'movie', 'shown', 'soon', 'old', 'of', 'explosion', 'action', 'include', \"let's\", 'as', 'on', 'it', 'is', 'americans', 'so', 'jean', 'pedestrian', 'for', 'there', 'housework', 'only', 'acting', 'or', 'of', 'writer', 'was', 'seeking', 'directed', 'moving', 'that', 'started', 'chen', 'without', 'wwii', 'to', 'perception', 'suffers', 'deputies', 'it', 'is', 'puzzled', 'stationary', 'suspects', 'deal', 'rated', 'is', 'belle', 'seasons', 'commemorated', 'need', 'i', 'i', 'known', 'dvd', 'have', 'he', 'early', 'to', 'actress', 'some', 'in', 'note', \"it's\", 'still', 'worst', 'realistic', 'as', 'on', 'garbage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Cleaning: Stopwords"
      ],
      "metadata": {
        "id": "BNmlmnWmH9hA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed these kind of cleanings in this part.\n",
        "- Remove the stopwords from the words\n",
        "- Remove puncuations\n",
        "- Remove numbers\n",
        "- Remove words with length of < 2\n"
      ],
      "metadata": {
        "id": "mCKc4NV4UMGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stopwords cleaning\n",
        "new_sentences = []\n",
        "for sentence in sentences:\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokenized = tokenizer.tokenize(' '.join(sentence))\n",
        "  clean_stop_words = [word for word in tokenized if word not in stop_words and len(word) > 2]\n",
        "  new_sentences.append(clean_stop_words)\n",
        "# Test stopwords\n",
        "print(new_sentences[200])\n",
        "\n",
        "\n",
        "new_test_sentences = []\n",
        "for sentence in test_sentences:\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokenized = tokenizer.tokenize(' '.join(sentence))\n",
        "  clean_stop_words = [word for word in tokenized if word not in stop_words and len(word) > 2]\n",
        "  new_test_sentences.append(clean_stop_words)\n",
        "# Test stopwords\n",
        "print(new_test_sentences[200])\n",
        "\n",
        "sentences = new_sentences\n",
        "test_sentences = new_test_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-xfiEIbIL6D",
        "outputId": "e7ef1abe-19d5-4687-e52b-7e1b5a3a9dc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['far', 'unless', 'since', 'quite', 'dialog', 'depicts', 'stupidity', 'robert', 'jack', 'quite', 'provides', 'island', 'plots', 'indicitive', 'gets', 'good', 'shallow', 'earlier', 'understand', 'movies', 'suckers', 'better', 'tupac', 'catch', 'clash', 'change', 'cartoon', 'questionable', 'actors', 'work', 'excellent', 'instead', 'ended', 'comparison', 'together', 'whether', 'zeta', 'hitchhiker', 'forum', 'plan', 'tragic', 'hard', 'abysmal', 'scene', 'give', 'within', 'telling', 'boring', 'see', 'glass', 'movie', 'culprit', 'anyone', 'nananana', 'good', 'hanging', 'marvellous', 'final', 'sister', 'could', 'film', 'levels', 'regards', 'western', 'else', 'could', 'totally', 'spin']\n",
            "['namely', 'interesting', 'worst', 'funny', 'lover', 'despite', 'decent', 'punk', 'dance', 'transfer', 'clear', 'sickeningly', 'ways', 'saw', 'cheap', 'freaky', 'movie', 'get', 'tactic', 'movie', 'shown', 'soon', 'old', 'explosion', 'action', 'include', 'let', 'americans', 'jean', 'pedestrian', 'housework', 'acting', 'writer', 'seeking', 'directed', 'moving', 'started', 'chen', 'without', 'wwii', 'perception', 'suffers', 'deputies', 'puzzled', 'stationary', 'suspects', 'deal', 'rated', 'belle', 'seasons', 'commemorated', 'need', 'known', 'dvd', 'early', 'actress', 'note', 'still', 'worst', 'realistic', 'garbage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXfDtZQhINU"
      },
      "source": [
        "### 2.4 Cleaning: Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used stemming to have better initial data."
      ],
      "metadata": {
        "id": "V5jOGTBlUaY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming cleaning\n",
        "new_sentences = []\n",
        "st = PorterStemmer()\n",
        "for sentence in sentences:\n",
        "  clean_stemming = [st.stem(word) for word in sentence]\n",
        "  new_sentences.append(clean_stemming)\n",
        "# Test stemming\n",
        "print(new_sentences[200])\n",
        "\n",
        "sentences = new_sentences\n",
        "\n",
        "\n",
        "# Apply stemming cleaning\n",
        "new_test_sentences = []\n",
        "st = PorterStemmer()\n",
        "for sentence in test_sentences:\n",
        "  clean_stemming = [st.stem(word) for word in sentence]\n",
        "  new_test_sentences.append(clean_stemming)\n",
        "# Test stemming\n",
        "print(new_test_sentences[200])\n",
        "\n",
        "test_sentences = new_test_sentences"
      ],
      "metadata": {
        "id": "SP2ikpxZhOJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235d5996-a196-431c-ae59-3e331a617b0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['far', 'unless', 'sinc', 'quit', 'dialog', 'depict', 'stupid', 'robert', 'jack', 'quit', 'provid', 'island', 'plot', 'indicit', 'get', 'good', 'shallow', 'earlier', 'understand', 'movi', 'sucker', 'better', 'tupac', 'catch', 'clash', 'chang', 'cartoon', 'question', 'actor', 'work', 'excel', 'instead', 'end', 'comparison', 'togeth', 'whether', 'zeta', 'hitchhik', 'forum', 'plan', 'tragic', 'hard', 'abysm', 'scene', 'give', 'within', 'tell', 'bore', 'see', 'glass', 'movi', 'culprit', 'anyon', 'nananana', 'good', 'hang', 'marvel', 'final', 'sister', 'could', 'film', 'level', 'regard', 'western', 'els', 'could', 'total', 'spin']\n",
            "['name', 'interest', 'worst', 'funni', 'lover', 'despit', 'decent', 'punk', 'danc', 'transfer', 'clear', 'sickeningli', 'way', 'saw', 'cheap', 'freaki', 'movi', 'get', 'tactic', 'movi', 'shown', 'soon', 'old', 'explos', 'action', 'includ', 'let', 'american', 'jean', 'pedestrian', 'housework', 'act', 'writer', 'seek', 'direct', 'move', 'start', 'chen', 'without', 'wwii', 'percept', 'suffer', 'deputi', 'puzzl', 'stationari', 'suspect', 'deal', 'rate', 'bell', 'season', 'commemor', 'need', 'known', 'dvd', 'earli', 'actress', 'note', 'still', 'worst', 'realist', 'garbag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I needed to update my mappers, I mean `word_index` and `invrt_word_index`."
      ],
      "metadata": {
        "id": "rhRPrBhqUbQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = {tokenizer.tokenize(word)[0]: i for word, i in word_index.items() if len(word) > 2 and len(tokenizer.tokenize(word)) != 0}\n",
        "invtr_word_index = {i: tokenizer.tokenize(word)[0] for word, i in word_index.items() if len(word) > 2 and len(tokenizer.tokenize(word)) != 0}\n",
        "\n",
        "word_index = {st.stem(word): i for word, i in word_index.items()}\n",
        "invtr_word_index = {i: st.stem(word) for word, i in word_index.items()}"
      ],
      "metadata": {
        "id": "JibaUL67Eis3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd45e56-02ad-46fe-9731-68a91a1e19cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Convert and replace all train data"
      ],
      "metadata": {
        "id": "bSVVXzbZNQSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = sen_index(word_index, sentences)\n",
        "print(x_train[200])\n",
        "\n",
        "x_test = sen_index(word_index, test_sentences)\n",
        "print(x_test[200])"
      ],
      "metadata": {
        "id": "kPg6UctkNeON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67234f7c-cf7c-47ab-b9d3-024e11ed335c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79864, 891, 72270, 18399, 80925, 2820, 22992, 30465, 39159, 18399, 29976, 46060, 6516, 78177, 211, 39801, 17284, 905, 7729, 62986, 71633, 68513, 25904, 49676, 20584, 1440, 51005, 7530, 88284, 12218, 16082, 302, 4158, 5716, 32034, 723, 8721, 33228, 20563, 2461, 87492, 43774, 4363, 62134, 66156, 743, 713, 1097, 1082, 5096, 62986, 12445, 256, 85847, 39801, 46822, 9844, 51436, 33700, 50254, 1420, 48867, 2901, 16099, 32407, 50254, 71763, 12424]\n",
            "[770, 925, 86059, 27562, 26235, 463, 539, 72039, 4200, 17505, 23339, 16117, 50682, 30241, 18900, 6585, 62986, 211, 38452, 62986, 614, 512, 30661, 6977, 83700, 1923, 3183, 87963, 1557, 39419, 23464, 914, 83414, 4843, 34510, 725, 39343, 10267, 85454, 85515, 82879, 51369, 30455, 6787, 23203, 25419, 14868, 5185, 11692, 86452, 79021, 7643, 85126, 285, 81891, 29182, 3209, 9678, 86059, 42268, 76123]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imQqe33JeZqS"
      },
      "source": [
        "## 3. Build Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opNqhDhai4oJ"
      },
      "source": [
        "### 3.1 Uni-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2LltG5vQjGJv"
      },
      "outputs": [],
      "source": [
        "positive_unigram = []\n",
        "negative_unigram = []\n",
        "\n",
        "# Length and order of these variables are the same\n",
        "#   - sentences\n",
        "#   - x_train\n",
        "#   - y_train\n",
        "for idx in range(len(x_train)):\n",
        "  res = y_train[idx]\n",
        "  unigrams = ngrams(deepcopy(sentences[idx]), 1)\n",
        "  if res == 1:\n",
        "    # It is positive comment\n",
        "    positive_unigram.extend(unigrams)\n",
        "  elif res == 0:\n",
        "    # It is a negative comment\n",
        "    negative_unigram.extend(unigrams)\n",
        "  else: \n",
        "    raise Exception(\"undefined y for this dataset\")\n",
        "\n",
        "# For laplacian smoothing\n",
        "positive_vocab_size = len(collections.Counter(positive_unigram))\n",
        "negative_vocab_size = len(collections.Counter(negative_unigram))\n",
        "\n",
        "# Lengths\n",
        "len_positive_unigram = len(positive_unigram)\n",
        "len_negative_unigram = len(negative_unigram)\n",
        "\n",
        "# Laplacian smoothing\n",
        "count_pos_unigram = dict(collections.Counter(positive_unigram))\n",
        "count_neg_unigram = dict(collections.Counter(negative_unigram))\n",
        "\n",
        "# <UNK> in .get()\n",
        "unigram = {\n",
        "  'pos': {v: (count_pos_unigram.get((k,), 0) + 1)/(len_positive_unigram + positive_vocab_size) for k, v in word_index.items()},\n",
        "  'neg': {v: (count_neg_unigram.get((k,), 0) + 1)/(len_negative_unigram + negative_vocab_size) for k, v in word_index.items()}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.1 Evaluation"
      ],
      "metadata": {
        "id": "XxSrg4SmqQ_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_initial = y_train[y_train==1].shape[0] / y_train.shape[0]\n",
        "pn_initial = y_train[y_train==0].shape[0] / y_train.shape[0]\n",
        "\n",
        "y_predict = []\n",
        "\n",
        "for n in range(len(x_test)):\n",
        "  w = x_test[n]\n",
        "  pp = pp_initial * 10**300\n",
        "  pn = pn_initial * 10**300\n",
        "\n",
        "  for i in range(min(160, len(w))):\n",
        "    pp *= unigram['pos'].get(w[i], 1)\n",
        "    pn *= unigram['neg'].get(w[i], 1)\n",
        "  y_predict.append(pp > pn)"
      ],
      "metadata": {
        "id": "CRoWtogs7D5V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == True and predict == True])\n",
        "fp = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == False and predict == True])\n",
        "fn = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == True and predict == False])\n",
        "tn = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == False and predict == False])\n",
        "\n",
        "accuracy = (tp + tn)/(tp + tn + fn + fp)\n",
        "print(\"Accuracy: \", accuracy * 100)\n",
        "precision = (tp)/(tp + fp)\n",
        "print(\"Precision: \", precision * 100)\n",
        "recall = (tp)/(tp + fn)\n",
        "print(\"Recall: \", recall * 100)\n",
        "print(\"F1-Score: \", (2 * precision * recall)/(precision + recall) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-NLEaZeVQx0",
        "outputId": "78224d2a-f82f-4eed-f9d9-1f5a49320d39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  79.384\n",
            "Precision:  83.63553113553114\n",
            "Recall:  73.064\n",
            "F1-Score:  77.99316823228011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I1UwalSjA63"
      },
      "source": [
        "### 3.2. Bi-Gram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J4nmoJX7jG8-"
      },
      "outputs": [],
      "source": [
        "positive_bigram = []\n",
        "negative_bigram = []\n",
        "\n",
        "# Length and order of these variables are the same\n",
        "#   - sentences\n",
        "#   - x_train\n",
        "#   - y_train\n",
        "for idx in range(len(x_train)):\n",
        "  res = y_train[idx]\n",
        "  biigrams = ngrams(deepcopy(sentences[idx]), 2)\n",
        "  if res == 1:\n",
        "    # It is positive comment\n",
        "    positive_bigram.extend(biigrams)\n",
        "  elif res == 0:\n",
        "    # It is a negative comment\n",
        "    negative_bigram.extend(biigrams)\n",
        "  else: \n",
        "    raise Exception(\"undefined y for this dataset\")\n",
        "\n",
        "# For laplacian smoothing\n",
        "positive_vocab_size = len(collections.Counter(positive_bigram))\n",
        "negative_vocab_size = len(collections.Counter(negative_bigram))\n",
        "\n",
        "# Lengths\n",
        "len_positive_bigram = len(positive_bigram)\n",
        "len_negative_bigram = len(negative_bigram)\n",
        "\n",
        "# Laplacian smoothing\n",
        "count_pos_bigram = dict(collections.Counter(positive_bigram))\n",
        "count_neg_bigram = dict(collections.Counter(negative_bigram))\n",
        "count_all_bigram = set(dict(collections.Counter(positive_bigram)).keys()) | set(dict(collections.Counter(negative_bigram)).keys())\n",
        "\n",
        "# <UNK> in .get()\n",
        "bigram = {\n",
        "  'pos': {(word_index.get(st.stem(k[0])), word_index.get(st.stem(k[1]))): (count_pos_bigram.get(k, 0) + 1)/(len_positive_bigram + len(count_all_bigram)) for k in count_all_bigram},\n",
        "  'neg': {(word_index.get(st.stem(k[0])), word_index.get(st.stem(k[1]))): (count_neg_bigram.get(k, 0) + 1)/(len_negative_bigram + len(count_all_bigram)) for k in count_all_bigram}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1 Evaluation"
      ],
      "metadata": {
        "id": "86kdkyO3qV76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_initial = y_train[y_train==1].shape[0] / y_train.shape[0]\n",
        "pn_initial = y_train[y_train==0].shape[0] / y_train.shape[0]\n",
        "\n",
        "y_predict = []\n",
        "\n",
        "for n in range(len(x_test)):\n",
        "  w = x_test[n]\n",
        "  pp = pp_initial * 10**300\n",
        "  pn = pn_initial * 10**300\n",
        "\n",
        "  for i in range(1, min(len(w), 150)):\n",
        "    pp *= bigram['pos'].get((w[i-1], w[i]), 1)\n",
        "    pn *= bigram['neg'].get((w[i-1], w[i]), 1)\n",
        "  y_predict.append(pp > pn)"
      ],
      "metadata": {
        "id": "mACUiXeVdJsz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == True and predict == True])\n",
        "fp = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == False and predict == True])\n",
        "fn = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == True and predict == False])\n",
        "tn = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == False and predict == False])\n",
        "\n",
        "accuracy = (tp + tn)/(tp + tn + fn + fp)\n",
        "print(\"Accuracy: \", accuracy * 100)\n",
        "precision = (tp)/(tp + fp)\n",
        "print(\"Precision: \", precision * 100)\n",
        "recall = (tp)/(tp + fn)\n",
        "print(\"Recall: \", recall * 100)\n",
        "print(\"F1-Score: \", (2 * precision * recall)/(precision + recall) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFGKX074dL7z",
        "outputId": "8fc145bc-4cf8-4a09-a3ec-27792c662758"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  80.536\n",
            "Precision:  85.18620943952803\n",
            "Recall:  73.92800000000001\n",
            "F1-Score:  79.15881445948261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCICu7qpjI9z"
      },
      "source": [
        "### 3.3. Tri-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t70q9YLTjIWD"
      },
      "outputs": [],
      "source": [
        "positive_trigram = []\n",
        "negative_trigram = []\n",
        "\n",
        "# Length and order of these variables are the same\n",
        "#   - sentences\n",
        "#   - x_train\n",
        "#   - y_train\n",
        "for idx in range(len(x_train)):\n",
        "  res = y_train[idx]\n",
        "  trigrams = ngrams(deepcopy(sentences[idx]), 3)\n",
        "  if res == 1:\n",
        "    # It is positive comment\n",
        "    positive_trigram.extend(trigrams)\n",
        "  elif res == 0:\n",
        "    # It is a negative comment\n",
        "    negative_trigram.extend(trigrams)\n",
        "  else: \n",
        "    raise Exception(\"undefined y for this dataset\")\n",
        "\n",
        "# For laplacian smoothing\n",
        "positive_vocab_size = len(collections.Counter(positive_trigram))\n",
        "negative_vocab_size = len(collections.Counter(negative_trigram))\n",
        "\n",
        "# Lengths\n",
        "len_positive_trigram = len(positive_trigram)\n",
        "len_negative_trigram = len(negative_trigram)\n",
        "\n",
        "# Laplacian smoothing\n",
        "count_pos_trigram = dict(collections.Counter(positive_trigram))\n",
        "count_neg_trigram = dict(collections.Counter(negative_trigram))\n",
        "count_all_trigram = set(dict(collections.Counter(positive_trigram)).keys()) | set(dict(collections.Counter(negative_trigram)).keys())\n",
        "\n",
        "# <UNK> in .get()\n",
        "trigram = {\n",
        "  'pos': {(word_index.get(st.stem(k[0])), word_index.get(st.stem(k[1])), word_index.get(st.stem(k[2]))): (count_pos_bigram.get(k, 0) + 1)/(len_positive_trigram + len(count_all_trigram)) for k in count_all_trigram},\n",
        "  'neg': {(word_index.get(st.stem(k[0])), word_index.get(st.stem(k[1])), word_index.get(st.stem(k[2]))): (count_neg_bigram.get(k, 0) + 1)/(len_negative_trigram + len(count_all_trigram)) for k in count_all_trigram}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.1 Evaluation"
      ],
      "metadata": {
        "id": "tcrv3aByqZJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_initial = y_train[y_train==1].shape[0] / y_train.shape[0]\n",
        "pn_initial = y_train[y_train==0].shape[0] / y_train.shape[0]\n",
        "\n",
        "y_predict = []\n",
        "\n",
        "for n in range(len(x_test)):\n",
        "  w = x_test[n]\n",
        "  pp = pp_initial * 10**300\n",
        "  pn = pn_initial * 10**300\n",
        "\n",
        "  for i in range(2, min(len(w), 160)):\n",
        "    pt = trigram['pos'].get((w[i-2], w[i-1], w[i]), 1)\n",
        "    pneg = trigram['neg'].get((w[i-2], w[i-1], w[i]), 1)\n",
        "\n",
        "    if pt == 1 or pneg == 1:\n",
        "      pt = 1\n",
        "      pneg = 1\n",
        "      a = sum([bigram['pos'].get((w[i-2], w[i-1]), 1), bigram['pos'].get((w[i-2], w[i]), 1), bigram['pos'].get((w[i-1], w[i]), 1)])/3\n",
        "      b = sum([bigram['neg'].get((w[i-2], w[i-1]), 1), bigram['neg'].get((w[i-2], w[i]), 1), bigram['neg'].get((w[i-1], w[i]), 1)])/3\n",
        "      if a != 1 and b != 1:\n",
        "        pt = a\n",
        "        pneg = b\n",
        "\n",
        "\n",
        "    pp *= pt\n",
        "    pn *= pneg\n",
        "\n",
        "  y_predict.append(pp > pn)"
      ],
      "metadata": {
        "id": "VNLdylgBjHWI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == True and predict == True])\n",
        "fp = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == False and predict == True])\n",
        "fn = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == True and predict == False])\n",
        "tn = len([(idx, predict) for idx, predict in enumerate(y_predict) if y_test[idx] == False and predict == False])\n",
        "\n",
        "accuracy = (tp + tn)/(tp + tn + fn + fp)\n",
        "print(\"Accuracy: \", accuracy * 100)\n",
        "precision = (tp)/(tp + fp)\n",
        "print(\"Precision: \", precision * 100)\n",
        "recall = (tp)/(tp + fn)\n",
        "print(\"Recall: \", recall * 100)\n",
        "print(\"F1-Score: \", (2 * precision * recall)/(precision + recall) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlU-MMNyjJ54",
        "outputId": "472e8d74-e1ea-48ae-9d03-66feaadcee50"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  74.896\n",
            "Precision:  81.58108382382788\n",
            "Recall:  64.312\n",
            "F1-Score:  71.9244877874206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Questions\n",
        "\n",
        "- I needed to multiply the probabilities to a large number because they became 0.\n",
        "- Yes we can see bag of words effect from the results.\n",
        "- I ran this model multiple time. I added stemming to have one single token for the same words. I removed the puncuation. I removed the number.\n",
        "- I got out of memory errror, so i had to do some optmization to ignore some considerations.\n",
        "- The notable number of tokens in test sentences are not available in my dictionay that caused some errors in the results. \n",
        "- The bigram turns out to be the best choice for classification and then unigram."
      ],
      "metadata": {
        "id": "CnhAZYtoU5PE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEQ3fUeuMdqz"
      },
      "source": [
        "## Good Luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}